{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Str, Bytes & bytesarray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chararcters Issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### what is a str ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "by definiton a sting is a sequence of characters. a character can be defined as a readable representation of word which is usually hexadicimal byte also called **Unicode or code point**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Unicode ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green;\">a character is firstly identified by a digit that ranges from 0 to 1,114,111  </span>\n",
    "\n",
    "A Unicode is a standard of representing a character with number ranging from 0 to 1,114,116 in a hexadecimal manner.For example, the letter A is:\n",
    "\n",
    "1. Represented by 0041 hexadicimal code \n",
    "2. This code is then appended to 'U+'\n",
    "3. It turns out to tbe U+0041"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How about python then ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For python, we use bytes or bytearray to have a byte representation of a string. The difference between them in python is bytes is immutable.\n",
    "\n",
    "to go from str (or from code point) to byte sequence, we use an encoding algo. Python has builtin algos for such purposes (utf-8, utf-16)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">  \n",
    "<b>Warning:</b> In python 2, the byte was an alias for str. It was not there before.  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "bytearray(b'caf\\xc3\\xa9')\n",
      "99\n",
      "bytearray(b'c')\n"
     ]
    }
   ],
   "source": [
    "my_str = 'café'\n",
    "print(len(my_str))\n",
    "\n",
    "my_byte = bytearray(my_str, encoding='utf-8')\n",
    "print(my_byte)\n",
    "# an element of bytearray will be an integer \n",
    "# the range is from 0 to 255\n",
    "print(my_byte[0])\n",
    "# a slice of bytearray will be bytearray \n",
    "print(my_byte[:1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Information:</b> when you print a byte sequence:\n",
    "    <ol>\n",
    "        <li>If the character is within the ASCII range: from empty space to ~, you will see the character instead of the byte</li>\n",
    "        <li>Other special characters like tab, newline, backslash and return will be not represented with bytes</li>\n",
    "        <li>Others on the other hand, we will see their byte representation</li>\n",
    "    </ol>\n",
    "    Also, the byte sequence type share the str methods that are not related to Unicode and Formatting like the following \n",
    "    <ul>\n",
    "        <li>format() and formatmapping()</li>\n",
    "        <li>asefold, isdecimal, isidentifier,isnumeric, isprintable, and encode</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different ways of creating bytesarray or bytes \n",
    "\n",
    "1. You can pass an object that support the buffer protocol (like arra)\n",
    "2. You can pass a string along with the encoding algo \n",
    "3. You can pass a string with hexadicimal digit sperated by space using the method fromhex()\n",
    "4. An iterable with integer ranging between 0 and 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Struct and MemoryViews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Struct is used for essentially two things: \n",
    "\n",
    "1. decompose byte sequence (bytes and bytearray) into tuple in which each element is a byte. \n",
    "2. It will also allow you to go from the tuple of bytes to the byte sequece too.\n",
    "\n",
    "When to use it:\n",
    "\n",
    "1. Dealing with binary files: you can read binary files and unpack those bytes through struct \n",
    "2. Network and Protocal: \n",
    "    * pack data when sending over the network \n",
    "    * unpack data when receivi the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, b'Hello')\n"
     ]
    }
   ],
   "source": [
    "import struct\n",
    "\n",
    "# Binary data containing an integer and a string\n",
    "packed_data = b'\\x01\\x00\\x00\\x00Hello'\n",
    "\n",
    "# Format string:\n",
    "# - 'i' for integer (4 bytes)\n",
    "# - '5s' for a string of 5 characters\n",
    "fmt = 'i5s'\n",
    "\n",
    "# Unpack the data\n",
    "unpacked_data = struct.unpack(fmt, packed_data)\n",
    "\n",
    "print(unpacked_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the packed data is the following b'\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\nHelloWorld'\n"
     ]
    }
   ],
   "source": [
    "import struct\n",
    "import socket\n",
    "\n",
    "# Example: Packing a message for a network protocol\n",
    "message_type = 1\n",
    "message_length = 10\n",
    "payload = b'HelloWorld'\n",
    "packed_data = struct.pack('!II10s', message_type, message_length, payload)\n",
    "\n",
    "print(f'the packed data is the following {packed_data}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoders And Decoders "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In python, we have arround 100 codecs (encoders/decoders), The famous are the following \n",
    "\n",
    "1. cp1252: the introduced by the miscrosoft OS. \n",
    "2. iso8859_1: it is the grand parents of most the codecs here \n",
    "3. UTF-8: most common one, that covers almost every character backward compatible with ASCII\n",
    "4. UTF-16LE: it covers almost everything "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Codecs Problems "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 3 types of CODECS problems:\n",
    "\n",
    "1. EnocdingError\n",
    "2. DecodingError\n",
    "3. Syntax Error \n",
    "\n",
    "most of the error usually comes from the first 2 cases "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding Error can come from:\n",
    "\n",
    "1. Having a character that is out of range of the codec you are using\n",
    "2. Not specifying the codec type (utf-8, cp1552,iso8859_1) when calling the encoder methods  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "utf-8 encoding is as the following : b'S\\xc3\\xa3o Paulo'\n",
      "utf-16LE encoding is as the following : b'\\xff\\xfeS\\x00\\xe3\\x00o\\x00 \\x00P\\x00a\\x00u\\x00l\\x00o\\x00'\n",
      "encodign with cp will not work as ã is out of range\n"
     ]
    },
    {
     "ename": "UnicodeEncodeError",
     "evalue": "'charmap' codec can't encode character '\\xe3' in position 1: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeEncodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-16LE encoding is as the following :\u001b[39m\u001b[38;5;124m\"\u001b[39m,city\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-16\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencodign with cp will not work as ã is out of range\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcp encoding is as the following :\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[43mcity\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcp437\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/lib/python3.10/encodings/cp437.py:12\u001b[0m, in \u001b[0;36mCodec.encode\u001b[0;34m(self, input, errors)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mencode\u001b[39m(\u001b[38;5;28mself\u001b[39m,\u001b[38;5;28minput\u001b[39m,errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m---> 12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcodecs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcharmap_encode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43mencoding_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mUnicodeEncodeError\u001b[0m: 'charmap' codec can't encode character '\\xe3' in position 1: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "city = 'São Paulo'\n",
    "print(\"utf-8 encoding is as the following :\",city.encode('utf-8'))\n",
    "\n",
    "print(\"utf-16LE encoding is as the following :\",city.encode('utf-16'))\n",
    "\n",
    "print(\"encodign with cp will not work as ã is out of range\")\n",
    "print(\"cp encoding is as the following :\",city.encode('cp437'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One solution is to use the error method that acts as a **catch** for the EncodingError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cp encoding is as the following : b'So Paulo'\n",
      "cp encoding is as the following : b'S?o Paulo'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"cp encoding is as the following :\",city.encode('cp437',errors='ignore'))\n",
    "\n",
    "print(\"cp encoding is as the following :\",city.encode('cp437', errors=\"replace\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding error "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not every byte holds a valid ASCII character, and not every byte\n",
    "sequence is valid UTF-8 or UTF-16; therefore, when you assume one\n",
    "of these encodings while converting a binary sequence to text, you will\n",
    "get a UnicodeDecodeError if unexpected bytes are found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">  \n",
    "<b>Warning:</b> Some Encoding will silently fails by either replacing it by garbage or something close (the case of cp1552 and iso8859_1).  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bytes'>\n",
      "Montréal\n",
      "Montrιal\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xe9 in position 5: invalid continuation byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(octets\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcp1252\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(octets\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124miso8859_7\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m----> 7\u001b[0m \u001b[43moctets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xe9 in position 5: invalid continuation byte"
     ]
    }
   ],
   "source": [
    "octets = b'Montr\\xe9al'\n",
    "print(type(octets))    \n",
    "\n",
    "print(octets.decode(\"cp1252\"))\n",
    "print(octets.decode('iso8859_7'))\n",
    "\n",
    "octets.decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "similarly, you can the error parameters to handle decodingError \n",
    "```python \n",
    "octets.decode('utf_8', errors='replace')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Olá, Mundo!\n"
     ]
    }
   ],
   "source": [
    "print('Olá, Mundo!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "café\n"
     ]
    }
   ],
   "source": [
    "s2 = 'cafe\\u0301'\n",
    "print(type(s2))\n",
    "# print(s2.decode(encoding=\"utf-8\"))\n",
    "print(s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABC\n",
      "ＡＢＣ\n"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "\n",
    "full_width = \"ＡＢＣ\"  # Full-width Latin letters (U+FF21, U+FF22, U+FF23)\n",
    "nfkc_text = unicodedata.normalize(\"NFKC\", full_width)\n",
    "nfc_text = unicodedata.normalize(\"NFC\",full_width)\n",
    "\n",
    "print(nfkc_text)\n",
    "print(nfc_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fi\n",
      "ﬁ\n"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "\n",
    "ligature = \"ﬁ\"  # Ligature \"fi\" (U+FB01)\n",
    "nfkc_text = unicodedata.normalize(\"NFKC\", ligature)\n",
    "nfc_text = unicodedata.normalize(\"NFC\",ligature)\n",
    "\n",
    "print(nfkc_text)  # \"fi\" (U+0066 + U+0069)\n",
    "print(nfc_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Syntax Error "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "when writing code in python 2, the default codec was ASCII and then from python we went to UTF-8 codec as default. And in OS like windows, the default codec was or is cp1552.\n",
    "This is important as when you **import** python module (in your linux OS) that have been written under such context (cp1552). You might face the **SYNTAXERROR** issue "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid this, at the beggining of the module you want to import, you add this comment in the first line of the module python file.\n",
    "\n",
    "```python \n",
    "\n",
    "# the_name_of_legacy_codec like below\n",
    "# cp_1552\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identiying the codec from a byte sequence "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ASCII vs UTF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The range value of ASCII is between 0 and 127, if you want a byte where its converted numerical value is not within that range, than you can assume it UTF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UTF-8 vs UTF-16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some bytes pattern that are indications of bugs in UTF-8 but not in UTF-16 like: '\\x00' . But under normal circumstance, it is difficult to prgramicitally to do so unless some assumption. Even with that, there is already a python library that does that. It is called **chardet**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Files "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When handling files with python, there a set of principle that is suggestd to do:\n",
    "\n",
    "1. Decoding the content ASAP (from byte to str)\n",
    "2. Handling the string, in your business logic (string manipulation) \n",
    "3. Encoding your text back As Late As Possible (from str to byte )\n",
    "\n",
    "This what we call the **Burger** Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">  \n",
    "<b>Warning:</b> You should never be encoding or decoding in the middle of other processing\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">  \n",
    "<b>Warning:</b> When openig a file for reading or writing provide the encoding so that you don't use the default encoding of your OS \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The default codec "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the default codec is usually attached to the OS system that you use. There are 4 default codec:\n",
    "\n",
    "1. Affecting how you open a file \n",
    "2. Affection your stdout,stdin,and stderror\n",
    "3. Internal conversion of your bytes to str and vice-versa \n",
    "4. Reading the name of the file you want to handle \n",
    "\n",
    "\n",
    "For Windows for example, you might have 4 different codecs. \n",
    "\n",
    "**MAKE SURE TO PRECISE WHAT CODEC YOU WANT TO USE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing Unicode "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When doing string comparaison, you may have two string when printed are the same but the way it is represented in the code is different. For example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s1 when printed café\n",
      "s2 when printed café\n",
      "the length of s1 4\n",
      "the length of s2 5\n",
      "are s1 and s2 equal False\n"
     ]
    }
   ],
   "source": [
    "s1 = \"café\"\n",
    "s2 = \"cafe\\u0301\"\n",
    "\n",
    "print(f's1 when printed {s1}')\n",
    "print(f's2 when printed {s2}')\n",
    "print(f'the length of s1 {len(s1)}')\n",
    "print(f'the length of s2 {len(s2)}')\n",
    "print(f'are s1 and s2 equal {s1==s2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what happened here is that there are two ways with UTF to represent **diacritics** (les accents[aigu,grave,circonflex])\n",
    "\n",
    "1. Embedded with the letter in one byte (1 bytes for both)\n",
    "2. 1 byte for the letter and 1 byte for the diacritic\n",
    "\n",
    "To make sure these words are equals, we need to normalise both strings "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NFC & NFD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NFC (Normalisation Form Compose)\n",
    "\n",
    "1. It will take the string as input, and try to make the **shortest** (compose) possible by embedding the diacritic with previous letter (if possible)\n",
    "2. It will select the prefered cannonical representation \n",
    "\n",
    "NFD (Normalisation Form Decompose)\n",
    "\n",
    "\n",
    "1. it will take the string as input, and decompose the embedded letters in one byte and output a **decomposed** string (if applicable)\n",
    "2. it will then select the prefered cannonical representation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "are s1 and s2 equal False\n",
      "s1_nfc when printed caf\\xe9\n",
      "s2_nfc when printed caf\\xe9\n",
      "s1_nfd when printed cafe\\u0301\n",
      "s2_nfd when printed cafe\\u0301\n"
     ]
    }
   ],
   "source": [
    "from unicodedata import normalize\n",
    "\n",
    "s1 = \"café\"\n",
    "s2 = \"cafe\\u0301\"\n",
    "\n",
    "print(f'are s1 and s2 equal {s1==s2}')\n",
    "\n",
    "# NFC \n",
    "s1_nfc = normalize('NFC', s1)\n",
    "s2_nfc = normalize('NFC', s2)\n",
    "\n",
    "#NFD\n",
    "s1_nfd = normalize('NFD',s1)\n",
    "s2_nfd = normalize('NFD', s2)\n",
    "\n",
    "print(f's1_nfc when printed {s1_nfc.encode(\"unicode_escape\").decode(\"ascii\")}')\n",
    "print(f's2_nfc when printed {s2_nfc.encode(\"unicode_escape\").decode(\"ascii\")}')\n",
    "\n",
    "print(f's1_nfd when printed {s1_nfd.encode(\"unicode_escape\").decode(\"ascii\")}')\n",
    "print(f's2_nfd when printed {s2_nfd.encode(\"unicode_escape\").decode(\"ascii\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">  \n",
    "<b>Warning:</b> Sometimes, both NFD and NFC lead to the same <b>code point result</b> when the decomp and comp are not applicable for the same string \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ω\n",
      "OHM SIGN\n",
      "Ω\n",
      "GREEK CAPITAL LETTER OMEGA\n",
      "the original OHM utf codepoint \\u2126 vs NFC \\u03a9\n",
      "the original OHM utf codepoint \\u2126 vs NFD \\u03a9\n"
     ]
    }
   ],
   "source": [
    "from unicodedata import normalize, name\n",
    "\n",
    "# in this code snippet NFC WILL SKIP STEP 1 and go directly to step 2 to find the standard canonical equivalence\n",
    "\n",
    "ohm = '\\u2126'\n",
    "print(ohm)\n",
    "print(name(ohm))\n",
    "\n",
    "ohm_c = normalize('NFC', ohm)\n",
    "ohm_c_nfd = normalize('NFD', ohm)\n",
    "print(ohm_c)\n",
    "print(name(ohm_c))\n",
    "print(f'the original OHM utf codepoint {ohm.encode(\"unicode_escape\").decode(\"ascii\")} vs NFC {ohm_c.encode(\"unicode_escape\").decode()}')\n",
    "print(f'the original OHM utf codepoint {ohm.encode(\"unicode_escape\").decode(\"ascii\")} vs NFD {ohm_c_nfd.encode(\"unicode_escape\").decode()}')\n",
    "# print(ohm.encode())\n",
    "#  unicode_escape = ohm_c.encode(\"unicode_escape\").decode(\"ascii\")\n",
    "# print(f\"The Unicode escape sequence for '{ohm_c}' is: {unicode_escape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NFKC & NFKD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NFKC and NFKD go beyond and stricter than their precursor. It will look further then their canonical equivalent :\n",
    "\n",
    "1. samestep1\n",
    "2. it will go through their meaning and funcionality of the string \n",
    "3. if it shares same functionality as the prefered one, then it will be converted to that "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the NFKD representation of the ½ is the 1⁄2\n",
      "the NFKC representation of ½ is the 1⁄2\n",
      "initial length 1\n",
      "the NFKC length is the following 3\n",
      "False\n",
      "1/2\n",
      "True\n",
      "the NFKC representation to four_squared is 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from unicodedata import normalize, name\n",
    "half = '½'\n",
    "\n",
    "nfkc_half = normalize('NFKC', half)\n",
    "nfkd_half = normalize('NFKD', half)\n",
    "\n",
    "print(f\"the NFKD representation of the ½ is the {nfkd_half}\")\n",
    "print(f\"the NFKC representation of ½ is the {nfkc_half}\")\n",
    "print(f\"initial length {len(half)}\")\n",
    "print(f\"the NFKC length is the following {len(nfkc_half)}\")\n",
    "\n",
    "print(nfkd_half==normalize('NFKC',\"1/2\"))\n",
    "print(normalize('NFKC',\"1/2\"))\n",
    "print(nfkd_half==nfkc_half)\n",
    "\n",
    "four_squared = '4²'\n",
    "four_squared_nfkc = normalize('NFKC', four_squared)\n",
    "print(f\"the NFKC representation to four_squared is {four_squared_nfkc}\")\n",
    "\n",
    "name(\"a\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the half string is decomposed to the functional equivalent without even caring about the formatting (like the case of 4 to the power of 2 conversted to 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">  \n",
    "<b>Warning:</b> Due to the strictness, and the format change that could occur with NFKC and NFKD, we should these tools carefully. It is usally used for indexing and search and <b> not permanent storage because of the data loss (format loss) </b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case folding "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Case folding the string equivalent of lower(). This will be applied correctly to most of the utf-8 characters by the exception of 116 character which 0.1% of the total cases. For example:\n",
    "\n",
    "1. The micro sign will be converted to the Greek letter mu \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the name of micro is: MICRO SIGN\n",
      "the name of the casefold of micro sign is the: GREEK SMALL LETTER MU\n"
     ]
    }
   ],
   "source": [
    "from unicodedata import name, normalize\n",
    "\n",
    "micro = 'µ'\n",
    "print(f'the name of micro is: {name(micro)}')\n",
    "\n",
    "micro_casefold = micro.casefold()\n",
    "print(f'the name of the casefold of micro sign is the: {name(micro_casefold)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can build uitility funciton to compare if two strings are equivalently equal with Normalisation and casefolding \n",
    "\n",
    "1. You normalise your string a and string b (to make sure the code points, which is the way comparaison is done, are equivalent)\n",
    "2. then casefold those strings to make sure they are case insensitive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unicodedata import normalize\n",
    "\n",
    "def utility_comp(s1,s2):\n",
    "    return (normalize('NFC', s1).casefold() == normalize('NFC', s2).casefold())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "s1 = 'café'\n",
    "s2 = 'cafe\\u0301'\n",
    "s3 = 'Straße'\n",
    "s4 = 'strasse'\n",
    "print(s3 == s4)\n",
    "print(utility_comp('Straße','strasse'))\n",
    "print(utility_comp(s1,s2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Diatcritics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can try removing diacritics of a string by doing the following \n",
    "\n",
    "1. Use NFD to decompose diacritics combined characters \n",
    "2. loop over the NFCed string \n",
    "3. if the character is a combined one, then ommit it. Otherwise add it to the string \n",
    "4. Use NFC, to compose back the string to its most composed way \n",
    "\n",
    "Why do we need that?\n",
    "\n",
    "1. You can make UTL more readable http://en.wikipedia.org/wiki/S%C3%A3o_Paulo to http://en.wikipedia.org/wiki/Sao_Paulo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cafe\n",
      "Sao Paulo\n"
     ]
    }
   ],
   "source": [
    "from unicodedata import normalize, combining\n",
    "\n",
    "def normalise_without_diacritics(s1):\n",
    "    decompose_string = normalize('NFD',s1)\n",
    "    result = ''\n",
    "    for c in decompose_string:\n",
    "        result = result if combining(c) else result + c\n",
    "    return normalize('NFC',result)\n",
    "\n",
    "print(normalise_without_diacritics('café'))\n",
    "print(normalise_without_diacritics('São Paulo'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extreme Removing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we want to remove diacritics for only alphabetical letter (letter in the ascii range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ζέφυρος, Zefiro\n",
      "Ζέφυρος, Zefirέ\n"
     ]
    }
   ],
   "source": [
    "from unicodedata import combining, normalize\n",
    "from string import ascii_letters\n",
    "\n",
    "def extreme_removing(s1):\n",
    "    normalised_str = normalize('NFD',s1)\n",
    "    previous_char_latin = False\n",
    "    result = ''\n",
    "    for c in normalised_str:\n",
    "        if combining(c) and previous_char_latin:\n",
    "            # previous_char_latin = False\n",
    "            # this one can be omitted because it will be update on the next character\n",
    "            continue \n",
    "        result = result + c\n",
    "        if not combining(c):\n",
    "            previous_char_latin = True if c in ascii_letters else False\n",
    "\n",
    "    return normalize('NFC',result)\n",
    "\n",
    "print(extreme_removing('Ζέφυρος, Zéfiro'))\n",
    "print(extreme_removing('Ζέφυρος, Zéfirέ'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ζέφυρος, Zefirέ\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import unicodedata\n",
    "def shave_marks_latin(txt):\n",
    "    \"\"\"Remove all diacritic marks from Latin base characters\"\"\"\n",
    "    norm_txt = unicodedata.normalize('NFD', txt)\n",
    "    latin_base = False\n",
    "    keepers = []\n",
    "    for c in norm_txt:\n",
    "        if unicodedata.combining(c) and latin_base:\n",
    "            continue # ignore diacritic on Latin base char\n",
    "        keepers.append(c)\n",
    "        # if it isn't combining char, it's a new base char\n",
    "        if not unicodedata.combining(c):\n",
    "            latin_base = c in string.ascii_letters\n",
    "    shaved = ''.join(keepers)\n",
    "    return unicodedata.normalize('NFC', shaved)\n",
    "    \n",
    "print(shave_marks_latin('Ζέφυρος, Zéfirέ'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unicode Sorting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a sequence of strings, the sorting will be based on the code point (utf-8) of each character.\n",
    "This can lead to potential problems when it comes to characters that are nto in the ascii range "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['acerola', 'atemoia', 'açaí', 'caju', 'cajá']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fruits = ['caju', 'atemoia', 'cajá', 'açaí', 'acerola']\n",
    "fruits.sort()\n",
    "fruits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">  \n",
    "<b>Warning:</b> AS you can see, the cajá should be sorted before caju due to the fact that the \"<b>á</b>\" which is placed beyond the ascii scope </b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard way to sort non-ASCII text in Python is to use the\n",
    "locale.strxfrm function which, according to the locale module\n",
    "docs, “transforms a string to one that can be used in locale-aware\n",
    "comparisons.”\n",
    "To enable locale.strxfrm, you must first set a suitable locale for\n",
    "your application, and pray that the OS supports it. On GNU/Linux\n",
    "(Ubuntu 14.04) with the pt_BR locale, the sequence of commands in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "Error",
     "evalue": "unsupported locale setting",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlocale\u001b[39;00m\n\u001b[1;32m      2\u001b[0m fruits \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcaju\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124matemoia\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcajá\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maçaí\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124macerola\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m----> 4\u001b[0m \u001b[43mlocale\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetlocale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocale\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLC_COLLATE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpt_BR.UTF-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m sorted_arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(fruits,key\u001b[38;5;241m=\u001b[39mlocale\u001b[38;5;241m.\u001b[39mstrxfrm)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(sorted_arr)\n",
      "File \u001b[0;32m/usr/lib/python3.10/locale.py:620\u001b[0m, in \u001b[0;36msetlocale\u001b[0;34m(category, locale)\u001b[0m\n\u001b[1;32m    617\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m locale \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(locale, _builtin_str):\n\u001b[1;32m    618\u001b[0m     \u001b[38;5;66;03m# convert to string\u001b[39;00m\n\u001b[1;32m    619\u001b[0m     locale \u001b[38;5;241m=\u001b[39m normalize(_build_localename(locale))\n\u001b[0;32m--> 620\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_setlocale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcategory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocale\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mError\u001b[0m: unsupported locale setting"
     ]
    }
   ],
   "source": [
    "import locale\n",
    "fruits = ['caju', 'atemoia', 'cajá', 'açaí', 'acerola']\n",
    "\n",
    "locale.setlocale(locale.LC_COLLATE, 'pt_BR.UTF-8')\n",
    "\n",
    "sorted_arr = sorted(fruits,key=locale.strxfrm)\n",
    "print(sorted_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not the recommended way of sorting because:\n",
    "\n",
    "1. locale is a global variable, if you change it you will affect other function within the same venv\n",
    "2. the second parameter naming will depend on the OS. The same name will work on LInux but not On Windows\n",
    "3. Even one name is compatible for 1 OS (LInux for example). It does not mean it is available in that OS system "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unicode Collision Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve the problem mentionned above, you can use a python package called **pyuca**.  \n",
    "when called for sorting str sequence, It will:\n",
    "\n",
    "1. the local will be guessed (abstraction)\n",
    "2. Sort the sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['açaí', 'acerola', 'atemoia', 'cajá', 'caju']\n"
     ]
    }
   ],
   "source": [
    "import pyuca\n",
    "\n",
    "fruits = ['caju', 'atemoia', 'cajá', 'açaí', 'acerola']\n",
    "collatort = pyuca.Collator()\n",
    "\n",
    "sorted_fruits = sorted(fruits, key=collatort.sort_key)\n",
    "print(sorted_fruits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unicode Database "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Unicode DB, will provide information about UTF characters:\n",
    "\n",
    "0. The mapping of the charcter to the code point \n",
    "1. the digit representation of it \n",
    "2. the relation with other characters (helpful for equivalence)\n",
    "3. the type of the character: decimal, numeric and so on \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Information:</b> when you print a byte sequence:\n",
    "That’s how the\n",
    "str methods isidentifier, isprintable, isdecimal, and\n",
    "isnumeric work. str.casefold also uses information from a\n",
    "Unicode table.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U+0031\t  1   \tre_dig\tisdig\tisnum\t 1.00\tDIGIT ONE\n",
      "U+00bc\t  ¼   \t-\t-\tisnum\t 0.25\tVULGAR FRACTION ONE QUARTER\n",
      "U+00b2\t  ²   \t-\tisdig\tisnum\t 2.00\tSUPERSCRIPT TWO\n",
      "U+0969\t  ३   \tre_dig\tisdig\tisnum\t 3.00\tDEVANAGARI DIGIT THREE\n",
      "U+136b\t  ፫   \t-\tisdig\tisnum\t 3.00\tETHIOPIC DIGIT THREE\n",
      "U+216b\t  Ⅻ   \t-\t-\tisnum\t12.00\tROMAN NUMERAL TWELVE\n",
      "U+2466\t  ⑦   \t-\tisdig\tisnum\t 7.00\tCIRCLED DIGIT SEVEN\n",
      "U+2480\t  ⒀   \t-\t-\tisnum\t13.00\tPARENTHESIZED NUMBER THIRTEEN\n",
      "U+3285\t  ㊅   \t-\t-\tisnum\t 6.00\tCIRCLED IDEOGRAPH SIX\n"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "re_digit = re.compile(r'\\d')\n",
    "sample = '1\\xbc\\xb2\\u0969\\u136b\\u216b\\u2466\\u2480\\u3285'\n",
    "for char in sample:\n",
    "    print('U+%04x' % ord(char),\n",
    "    char.center(6),\n",
    "    're_dig' if re_digit.match(char) else '-',\n",
    "    'isdig' if char.isdigit() else '-',\n",
    "    'isnum' if char.isnumeric() else '-',\n",
    "    format(unicodedata.numeric(char), '5.2f'),\n",
    "    unicodedata.name(char),\n",
    "    sep='\\t')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Duality with Bytes and Str API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not every built-in module in python will treat seq bytes and string the samewa. We will explore that duality with RegEx and OS function "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regex "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For RegEx, it can accept both **bytes and str**. However when it comes to bytes, if the byte **does not represent an ASCII character, it will not match with anything (words, decimal )**\n",
    "However, str will match "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text\n",
      " 'Ramanujan saw ௧௭௨௯ as 1729 = 1³ + 12³ = 9³ + 10³.'\n",
      "Numbers\n",
      " str : ['௧௭௨௯', '1729', '1', '12', '9', '10']\n",
      " bytes: [b'1729', b'1', b'12', b'9', b'10']\n",
      "Words\n",
      " str : ['Ramanujan', 'saw', '௧௭௨௯', 'as', '1729', '1³', '12³', '9³', '10³']\n",
      " bytes: [b'Ramanujan', b'saw', b'as', b'1729', b'1', b'12', b'9', b'10']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "re_numbers_str = re.compile(r'\\d+')\n",
    "re_words_str = re.compile(r'\\w+')\n",
    "re_numbers_bytes = re.compile(rb'\\d+')\n",
    "re_words_bytes = re.compile(rb'\\w+')\n",
    "text_str = (\"Ramanujan saw \\u0be7\\u0bed\\u0be8\\u0bef\"\n",
    "\" as 1729 = 1³ + 12³ = 9³ + 10³.\")\n",
    "text_bytes = text_str.encode('utf_8')\n",
    "print('Text', repr(text_str), sep='\\n ')\n",
    "print('Numbers')\n",
    "print(' str :', re_numbers_str.findall(text_str))\n",
    "print(' bytes:', re_numbers_bytes.findall(text_bytes))\n",
    "print('Words')\n",
    "print(' str :', re_words_str.findall(text_str))\n",
    "print(' bytes:', re_words_bytes.findall(text_bytes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tamil number is not deteced as word nor as decimal when it is a **Byte**. that is not the case for string "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Information:</b> You can make str behaves like bytes with RegEx by using <b>re.ASCII</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OS functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ζέφυρος, Zéfiro\n",
      "Ζέφυρος, Zéfiro\n",
      "False\n",
      "Ζ\n",
      "ε\n",
      "́\n",
      "φ\n",
      "υ\n",
      "ρ\n",
      "ο\n",
      "ς\n",
      ",\n",
      " \n",
      "Z\n",
      "e\n",
      "́\n",
      "f\n",
      "i\n",
      "r\n",
      "o\n"
     ]
    }
   ],
   "source": [
    "from unicodedata import normalize\n",
    "\n",
    "Greek = 'Ζέφυρος, Zéfiro'\n",
    "\n",
    "normailised_nfd = normalize('NFD',Greek)\n",
    "normailised_nfc = normalize('NFC',Greek)\n",
    "print(normailised_nfd)\n",
    "print(normailised_nfc)\n",
    "print(normailised_nfc==normailised_nfd)\n",
    "\n",
    "for c in normailised_nfd:\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import string\n",
    "def shave_marks(txt):\n",
    "    \"\"\"Remove all diacritic marks\"\"\"\n",
    "    norm_txt = unicodedata.normalize('NFD', txt)\n",
    "    shaved = ''.join(c for c in norm_txt\n",
    "          if not unicodedata.combining(c))\n",
    "    return unicodedata.normalize('NFC', shaved)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ζεφυρος, Zefiro\n"
     ]
    }
   ],
   "source": [
    "print(shave_marks(Greek))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"'\" in string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "açaí\n"
     ]
    }
   ],
   "source": [
    "my_s = 'açaí'\n",
    "\n",
    "print(unicodedata.normalize('NFKC', my_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ramanujan saw ௧௭௨௯ as 1729 = 1³ + 12³ = 9³ + 10³.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_str = (\"Ramanujan saw \\u0be7\\u0bed\\u0be8\\u0bef\"\n",
    "\" as 1729 = 1³ + 12³ = 9³ + 10³.\")\n",
    "\n",
    "text_str"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fluentPythonEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
